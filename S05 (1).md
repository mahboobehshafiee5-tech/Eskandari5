# جزوه دانشگاهی: مبانی و بهینه‌سازی برنامه‌نویسی موازی (نسخه بازنویسی‌شده و مبسوط)

**تهیه شده:  (بر اساس اسلایدها ارائه‌شده توسط Prof. Amin Eskandari، Department of Computer Engineering, IAU of Shiraz)  
**ویراستاران محتوا، کد و ویرایش علمی:** Hamid Namjoo & AmirHossein Hemati  
**اقتباس از اسلایدها:** برخی اسلایدها از ارائه‌های Prof. Todd C. Mowry و Brian Railing (Department of Computer Science, Carnegie Mellon University)، و Prof. Kayvon Fatahalian و Prof. Kunle Olukotun (Department of Computer Science, Stanford University) اقتباس شده است.  
**تاریخ تهیه:** November 24, 2025.

---

## بخش 1: مبانی برنامه‌نویسی موازی (Parallel Programming Basics)
**بر اساس فایل: s05_1_progbasics.pptx (58 اسلاید)**

این بخش به عنوان پایه‌ای برای درک برنامه‌نویسی موازی عمل می‌کند و مدل‌های اصلی، مراحل ایجاد برنامه‌های موازی، و چالش‌های وابستگی و همگام‌سازی را پوشش می‌دهد. هدف، فراهم کردن چارچوبی مفهومی است که خواننده بتواند از آن برای طراحی برنامه‌های موازی واقعی استفاده کند، بدون نیاز به منابع مقدماتی اضافی.

### 1.1. مرور مدل‌های برنامه‌نویسی موازی
در دنیای محاسبات موازی، انتخاب مدل برنامه‌نویسی نقش کلیدی در سادگی توسعه، عملکرد و مقیاس‌پذیری برنامه ایفا می‌کند. سه مدل اصلی عبارتند از:

- **مدل آدرس فضایی مشترک (Shared Address Space):** در این مدل، ارتباط بین threads یا processes از طریق دسترسی مستقیم به حافظه مشترک انجام می‌شود و بدون ساختار خاصی (unstructured) و ضمنی (implicit) در عملیات لود (load) و استور (store) به متغیرهای مشترک صورت می‌گیرد. این مدل روشی طبیعی و شهودی برای برنامه‌نویسی است، زیرا شبیه به برنامه‌نویسی последовательی عمل می‌کند و نیاز به مدیریت صریح ارتباط را کاهش می‌دهد. با این حال، خطر "شلیک به پای خود" (shooting yourself in the foot) وجود دارد: برنامه ممکن است از نظر منطقی درست باشد، اما به دلیل ناهماهنگی‌های پنهان در دسترسی به داده‌ها، عملکرد ضعیفی نشان دهد. برای مثال، در محیط‌هایی مانند OpenMP یا pthreads، برنامه‌نویس باید مراقب race conditionها باشد.

- **مدل گذرنده پیام (Message Passing):** بر خلاف مدل قبلی، تمام ارتباطات به صورت ساختاربندی‌شده و صریح (explicit) از طریق ارسال و دریافت پیام‌ها (messages) انجام می‌شود. این مدل اغلب سخت‌تر است برای دستیابی به اولین نسخه درست برنامه، زیرا برنامه‌نویس باید جریان داده‌ها را به طور دقیق مدیریت کند. اما ساختار آن در دستیابی به برنامه‌های مقیاس‌پذیر و کارآمد بسیار مفید است، زیرا ارتباط را محدود و قابل کنترل می‌کند. مدل MPI (Message Passing Interface) نمونه کلاسیک آن است، که در سیستم‌های توزیع‌شده مانند کلاستری‌ها کاربرد دارد.

- **مدل موازی داده (Data Parallel):** در این مدل، محاسبات به عنوان یک عملیات "نقشه‌برداری" (map) بزرگ بر روی یک مجموعه داده ساختاربندی می‌شود، جایی که عملیات یکسانی بر عناصر مستقل داده اعمال می‌گردد. این مدل فرض می‌کند آدرس فضایی مشترکی برای لود ورودی‌ها و استور نتایج وجود دارد، اما ارتباط بین تکرارهای map را به شدت محدود می‌کند تا پردازش مستقل هر تکرار حفظ شود (هدف: جلوگیری از وابستگی‌های ناخواسته). تجسم‌های مدرن این مدل (مانند CUDA یا OpenCL) این ساختار را تشویق می‌کنند اما اجبار نمی‌کنند، و primitives اضافی برای الگوهای ارتباطی محدود ارائه می‌دهند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 2: نمودار مقایسه سه مدل (Shared Address Space، Message Passing، Data Parallel) اینجا درج شود.]

### 1.2. تمرین مدرن: برنامه‌نویسی ترکیبی
در عمل واقعی سیستم‌های محاسباتی بزرگ (مانند ابررایانه‌ها یا کلاستری‌ها)، مدل‌های خالص کمتر استفاده می‌شوند و رویکردهای ترکیبی غالب هستند. برای مثال، درون یک نود چند‌هسته‌ای (multi-core node) از مدل Shared Address Space (مانند OpenMP) برای بهره‌برداری از راحتی دسترسی مستقیم به حافظه استفاده می‌شود، در حالی که بین نودها از Message Passing (مانند MPI) برای ارتباط صریح و کارآمد بهره برده می‌شود. این ترکیب بسیار رایج است، زیرا Shared Address Space را جایی که پیاده‌سازی کارآمد است (درون نود، با latency پایین حافظه) به کار می‌گیرد و ارتباط صریح را در جاهایی که لازم است (بین نودها، با overhead شبکه) اعمال می‌کند.

علاوه بر این، مدل‌های Data-Parallel-ish (مانند CUDA/OpenCL) primitives همگام‌سازی سبک Shared-Memory را درون کرنل‌ها (kernels) پشتیبانی می‌کنند و اشکال محدودی از ارتباط بین-تکراری (inter-iteration communication) را مجاز می‌دانند، مانند shared memory محلی در یک block thread. این انعطاف‌پذیری اجازه می‌دهد تا برنامه‌نویسان از مزایای هر مدل بدون محدودیت‌های سخت استفاده کنند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 3: تجسم مدرن CUDA/OpenCL با shared memory درون coreها اینجا درج شود.]

### 1.3. ایجاد یک برنامه موازی: فرآیند فکری و مراحل کلیدی
ایجاد یک برنامه موازی فرآیندی سیستماتیک است که از شناسایی پتانسیل موازی‌سازی شروع می‌شود و به مدیریت پیچیدگی‌های عملی ختم می‌گردد. فرآیند فکری اصلی شامل سه گام کلیدی است:

1. **شناسایی کارهای موازی‌پذیر:** تعیین بخش‌هایی از برنامه که می‌توانند بدون وابستگی مستقیم (independent) اجرا شوند. این گام نیاز به تحلیل وابستگی‌های داده و کنترل دارد.
2. **پارتیشن‌بندی کار و داده:** تقسیم مسئله به زیرمسائل (subproblems) و تخصیص داده‌های مرتبط، با توجه به حفظ locality و کاهش ارتباط.
3. **مدیریت دسترسی، ارتباط و همگام‌سازی:** اطمینان از اینکه threads بدون تداخل (conflict-free) به داده‌ها دسترسی داشته باشند، ارتباط لازم را انجام دهند، و وابستگی‌ها را با primitives مناسب (مانند locks یا barriers) حفظ کنند.

هدف اصلی این فرآیند دستیابی به speedup است، که برای یک محاسبه ثابت تعریف می‌شود به عنوان:
\[
\text{Speedup}(P \text{ processors}) = \frac{\text{Time}(1 \text{ processor})}{\text{Time}(P \text{ processors})}
\]
اهداف جانبی شامل کارایی بالا در هزینه، مساحت، توان مصرفی، یا حل مسائل بزرگ‌تر از ظرفیت یک ماشین واحد است. بدون مدیریت دقیق، موازی‌سازی می‌تواند به جای speedup، overhead ایجاد کند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 4: نمودار speedup و اهداف جانبی اینجا درج شود.]

### 1.4. مسئولیت‌ها در ایجاد برنامه موازی: تقسیم وظایف بین برنامه‌نویس و سیستم
مسئولیت‌های اصلی – decomposition، assignment، orchestration، و mapping – می‌توانند توسط برنامه‌نویس، سیستم (کامپایلر، runtime، سخت‌افزار)، یا ترکیبی از آن‌ها بر عهده گرفته شوند. این تقسیم انعطاف‌پذیری ایجاد می‌کند:

- **Decomposition:** شکستن مسئله به tasks موازی (subproblems یا "کارهای انجام‌شدنی").
- **Assignment:** اختصاص tasks به threads (workers یا "کارگران").
- **Orchestration:** ساختار ارتباط، همگام‌سازی، سازماندهی داده‌ها در حافظه، و زمان‌بندی tasks.
- **Mapping:** نگاشت threads به واحدهای اجرایی سخت‌افزاری.

این مراحل از مسئله اولیه به اجرای نهایی روی ماشین موازی منتهی می‌شوند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 5: نمودار کامل Decomposition → Subproblems → Assignment → Parallel Threads → Orchestration → Parallel Program → Mapping → Execution on Parallel Machine اینجا درج شود.]

### 1.5. Decomposition: شکستن مسئله به tasks موازی
Decomposition هسته برنامه‌نویسی موازی است و شامل تقسیم مسئله به tasks است که می‌توانند همزمان (concurrently) اجرا شوند. این تقسیم لزوماً استاتیک نیست؛ tasks جدید می‌توانند در حین اجرا (dynamically) شناسایی شوند، مانند در الگوریتم‌های divide-and-conquer. ایده اصلی ایجاد حداقل تعداد tasks کافی برای اشغال تمام واحدهای اجرایی ماشین (execution units) است – مثلاً اگر 8 هسته داشته باشیم، حداقل 8 task مستقل نیاز است، اما برای تعادل بهتر، بیشتر (parallel slack) توصیه می‌شود.

جنبه کلیدی: شناسایی وابستگی‌ها (dependencies) – یا دقیق‌تر، عدم وجود آن‌ها. وابستگی داده (data dependency) زمانی رخ می‌دهد که یک task به خروجی task دیگری وابسته باشد، که موازی‌سازی را محدود می‌کند. ابزارهایی مانند graph dependency برای تحلیل کمک می‌کنند.

### 1.6. قانون Amdahl: محدودیت speedup توسط وابستگی‌های سریال
قانون Amdahl یکی از اصول بنیادین است که نشان می‌دهد حتی بخش‌های کوچک سریال، speedup کلی را محدود می‌کنند. فرض کنید S = کسر اجرای последовательی که ذاتاً سریال است (به دلیل وابستگی‌های اجتناب‌ناپذیر، مانند I/O یا initialization). آنگاه، حداکثر speedup روی P processors برابر است با:
\[
\text{Speedup} \leq \frac{1}{S + \frac{1-S}{P}}
\]
برای مثال، اگر S=0.05 (5% سریال)، speedup حداکثر حدود 19x است، حتی با P=∞. این قانون تأکید می‌کند که تمرکز روی موازی‌سازی بخش‌های بزرگ (parallelizable fraction) حیاتی است.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 7: نمودار Amdahl’s Law با مثال S=0.05 و منحنی speedup vs P اینجا درج شود.]

### 1.7. مثال ساده: محاسبه دو-گامی روی تصویر N x N
برای درک عملی، یک مثال کلاسیک را بررسی کنیم: محاسبه روی یک تصویر N x N با دو گام.

- **گام 1: دوبرابر کردن روشنایی تمام پیکسل‌ها.** این گام کاملاً مستقل است (هر پیکسل بدون وابستگی به دیگران محاسبه می‌شود)، زمان ~N².
- **گام 2: محاسبه میانگین تمام مقادیر پیکسل.** این گام نیاز به جمع کل دارد، که ذاتاً وابسته است.

**پیاده‌سازی последовательی:** هر گام ~N² زمان می‌گیرد، مجموع ~2N².

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 8: جدول زمان و موازی‌سازی برای sequential (N² + N² = 2N²) اینجا درج شود.]

**اولین تلاش موازی (P processors):** گام 1 را موازی کنید (زمان N²/P)، گام 2 سریال نگه دارید (N²). speedup کلی ≤ 2، زیرا بخش دوم سریال bottleneck است. این نشان‌دهنده S≈0.5 است.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 9: جدول مقایسه sequential vs parallel اولیه (N² + N²/P) اینجا درج شود.]

**موازی‌سازی گام 2:** جمع‌های جزئی (partial sums) را موازی محاسبه کنید (N²/P)، سپس سریال ترکیب (P). زمان گام 2: N²/P + P. وقتی N >> P، speedup ≈ P. سربار ترکیب جزئی (tree reduction) را در نظر بگیرید.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 10: جدول موازی‌سازی گام 2 با partial sums و overhead P اینجا درج شود.]

### 1.8. کاربرد Assignment استاتیک (ادامه از Decomposition به Assignment)
پس از decomposition، assignment کارها به threads انجام می‌شود. در assignment استاتیک، اختصاص از پیش تعیین‌شده است (هرچند نه لزوماً compile-time؛ می‌تواند به runtime parameters مانند اندازه داده یا تعداد threads وابسته باشد). این روش ساده است و سربار runtime کمی دارد، زیرا indexing ساده (مانند blocked یا interleaved) استفاده می‌شود. مثال: در solver شبکه، اختصاص سلول‌های برابر به threads.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 13: نمودار Assignment با subproblems به parallel threads اینجا درج شود.]

(توضیحات assignment در بخش‌های بعدی گسترش می‌یابد، اما اینجا به عنوان پلی به orchestration عمل می‌کند.)

### 1.9. Assignment: اختصاص tasks به threads (workers)
Assignment شامل نگاشت tasks (چیزهایی برای انجام) به threads (workers یا "کارگران") است. اهداف دوگانه: (1) تعادل بار کاری برای جلوگیری از idle threads، (2) کاهش هزینه‌های ارتباط (communication costs) با حفظ locality داده. assignment می‌تواند استاتیک (پیش‌تعیین‌شده) یا دینامیک (در runtime بر اساس رفتار) باشد. در حالی که برنامه‌نویس اغلب decomposition را مدیریت می‌کند، زبان‌ها و runtimeها (مانند ISPC یا Cilk) assignment را بر عهده می‌گیرند تا تعادل خودکار فراهم شود.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 14: مثال‌های assignment در ISPC (static vs system-managed) اینجا درج شود.]

#### 1.9.1. مثال‌های Assignment در ISPC
ISPC (Intel SPMD Program Compiler) ابزاری قدرتمند برای برنامه‌نویسی SIMD/MIMD است. 

- **Static Assignment (مدیریت‌شده توسط برنامه‌نویس):** decomposition بر اساس تکرار حلقه، assignment interleaved به program instances.
  ```cpp
  export void sinx(uniform int N, uniform int terms, uniform float* x, uniform float* result) {
      // فرض N % programCount = 0 برای تعادل
      for (uniform int i = 0; i < N; i += programCount) {
          int idx = i + programIndex;  // هر instance بخشی از حلقه را می‌گیرد
          float value = x[idx];
          float numer = x[idx] * x[idx] * x[idx];  // محاسبه sin(x) تقریبی با Taylor
          uniform int denom = 6;  // 3!
          uniform int sign = -1;
          for (uniform int j = 1; j <= terms; j++) {
              value += sign * numer / denom;
              numer *= x[idx] * x[idx];
              denom *= (2 * j + 2) * (2 * j + 3);
              sign *= -1;
          }
          result[idx] = value;  // ذخیره نتیجه در موقعیت محلی
      }
  }
  ```
  این روش تعادل را تضمین می‌کند اگر کار یکنواخت باشد، اما انعطاف‌پذیری کمی برای imbalance دارد.

- **System-Managed Assignment:** استفاده از foreach برای expose کار مستقل، سیستم assignment را مدیریت می‌کند (در ISPC فعلی استاتیک، اما پتانسیل دینامیک).
  ```cpp
  export void sinx(uniform int N, uniform int terms, uniform float* x, uniform float* result) {
      foreach (i = 0 ... N) {  // سیستم i را بین instances تقسیم می‌کند
          float value = x[i];
          float numer = x[i] * x[i] * x[i];
          uniform int denom = 6;  // 3!
          uniform int sign = -1;
          for (uniform int j = 1; j <= terms; j++) {
              value += sign * numer / denom;
              numer *= x[i] * x[i];
              denom *= (2 * j + 2) * (2 * j + 3);
              sign *= -1;
          }
          result[i] = value;
      }
  }
  ```
  این abstraction اجازه می‌دهد سیستم (مانند runtime) تعادل را بر اساس load فعلی تنظیم کند.

### 1.10. مثال Static Assignment با pthreads
pthreads (POSIX Threads) برای مدیریت threads در C/C++ استفاده می‌شود. مثال: محاسبه sinx موازی با blocked assignment.

```cpp
typedef struct { int N, terms; float* x, *result; } my_args;  // ساختار آرگومان برای thread

void parallel_sinx(int N, int terms, float* x, float* result) {
    pthread_t thread_id;  // ID thread جدید
    my_args args;
    args.N = N / 2; args.terms = terms; args.x = x; args.result = result;  // نیمه اول به thread جدید
    // راه‌اندازی thread دوم برای نیمه اول آرایه
    pthread_create(&thread_id, NULL, my_thread_start, &args);
    // main thread نیمه دوم را پردازش می‌کند
    sinx(N - args.N, terms, x + args.N, result + args.N);  // فراخوانی sequential sinx
    pthread_join(thread_id, NULL);  // منتظر تکمیل thread
}

void my_thread_start(void* thread_arg) {
    my_args* thread_args = (my_args*) thread_arg;
    sinx(thread_args->N, thread_args->terms, thread_args->x, thread_args->result);  // کار thread
}
```
- decomposition: تکرار حلقه؛ assignment: blocked (نیمه آرایه به هر thread). این روش locality خوبی برای cache فراهم می‌کند، اما اگر کار ناهموار باشد، imbalance ایجاد می‌کند.

### 1.11. Dynamic Assignment با ISPC Tasks
در سناریوهای unpredictable، dynamic assignment مفید است. در ISPC، launch tasks را ایجاد می‌کند.

```cpp
void foo(uniform float* input, uniform float* output, uniform int N) {
    // ایجاد 100 task مستقل
    launch[100] my_ispc_task(input, output, N);  // runtime tasks را به workers اختصاص می‌دهد
}
```
- policy: هر worker پس از task فعلی، next uncompleted را از لیست می‌گیرد. این روش تعادل را در runtime تضمین می‌کند، اما overhead queue management دارد.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 19: نمودار worker threads 0-3 و لیست tasks با next ptr اینجا درج شود.]

### 1.12. Orchestration: هماهنگی اجرای tasks
Orchestration لایه‌ای است که اجرای tasks را مدیریت می‌کند و شامل موارد زیر است:
- **ساختار ارتباط:** چگونگی انتقال داده بین threads (مثلاً shared variables یا messages).
- **همگام‌سازی برای وابستگی‌ها:** اضافه کردن locks یا barriers برای حفظ order.
- **سازماندهی داده در حافظه:** چیدمان آرایه‌ها برای locality (مثل padded structures برای cache lines).
- **زمان‌بندی tasks:** تصمیم‌گیری در مورد اجرای sequential vs parallel.

اهداف: کاهش latency ارتباط/همگام‌سازی، حفظ locality ارجاع داده (برای hit cache بالا)، و حداقل کردن overhead (مانند lock acquisition). تصمیمات به جزئیات ماشین وابسته است – مثلاً اگر sync گران است، از آن کمتر استفاده کنید.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 20: نمودار Orchestration با mapping به parallel program اینجا درج شود.]

### 1.13. Mapping به سخت‌افزار: نگاشت threads به منابع اجرایی
Mapping آخرین مرحله است و threads (workers) را به واحدهای اجرایی سخت‌افزاری (مانند cores یا lanes SIMD) نگاشت می‌کند. مثال‌ها:
- **توسط OS:** pthread به context HW روی core CPU (scheduler OS مدیریت می‌کند).
- **توسط کامپایلر:** program instances ISPC به lanes vector instructions.
- **توسط سخت‌افزار:** thread blocks CUDA به SMها (Streaming Multiprocessors) در GPU.

تصمیمات کلیدی:
- threads مرتبط (cooperating) روی همان processor برای حداکثر locality و اشتراک داده (حداقل هزینه sync/comm).
- threads نامرتبط روی همان processor برای بهره‌برداری از منابع (یکی compute-bound، دیگری memory-bound).

این نگاشت تأثیر مستقیمی بر عملکرد دارد، و runtimeها اغلب آن را پویا تنظیم می‌کنند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 22: نمودار Mapping به execution on parallel machine اینجا درج شود.]
[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 23: مثال‌های mapping (OS, compiler, hardware) اینجا درج شود.]

### 1.14. مثال برنامه‌نویسی موازی: حل‌کننده PDE روی شبکه 2D
برای کاربرد عملی، یک solver برای معادله دیفرانسیل جزئی (PDE) روی شبکه (N+2) x (N+2) را بررسی می‌کنیم. الگوریتم: حل تکراری Gauss-Seidel تا همگرایی، با به‌روزرسانی:
\[
A[i,j] = 0.2 \times (A[i,j] + A[i,j-1] + A[i-1,j] + A[i,j+1] + A[i+1,j])
\]
این الگوریتم وابستگی محلی دارد، که موازی‌سازی را چالش‌برانگیز می‌کند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 24: مثال کلی parallel programming اینجا درج شود.]
[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 25: شبکه N x N با IX(i,j) indexing اینجا درج شود.]

**پیاده‌سازی последователь (C-like pseudocode):**
```cpp
const int n;  // اندازه شبکه
float* A;  // آرایه (N+2) x (N+2)

void solve(float* A) {
    float diff, prev;
    bool done = false;
    while (!done) {
        diff = 0.0f;
        for (int i = 1; i <= n; i++) {  // حاشیه‌ها fixed
            for (int j = 1; j <= n; j++) {
                prev = A[IX(i,j)];  // IX: flatten 2D to 1D
                A[IX(i,j)] = 0.2f * (A[IX(i,j)] + A[IX(i-1,j)] + A[IX(i+1,j)] + 
                                     A[IX(i,j-1)] + A[IX(i,j+1)]);
                diff += fabs(A[IX(i,j)] - prev);  // تغییر برای همگرایی
            }
        }
        if (diff / (n * n) < TOLERANCE) done = true;  // شرط توقف
    }
}
```
این نسخه ساده است، اما موازی نیست.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 26: کد sequential solver با while loop و nested for اینجا درج شود.]

### 1.15. solver در مدل Shared Address Space (SPMD)
در SPMD (Single Program Multiple Data)، تمام threads همان کد را اجرا می‌کنند اما روی داده‌های متفاوت. برای 4 threads:

- **Blocked Assignment:** هر thread بخشی پیوسته از شبکه را مدیریت می‌کند (locality خوب، اما edge dependencies).

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 27: 4 threads با blocked rows (T0: rows 1-4, etc.) اینجا درج شود.]

- **Interleaved Assignment:** threads سطرهای متناوب را پردازش می‌کنند (تعادل بهتر، اما locality ضعیف‌تر).

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 28: 4 threads با interleaved rows (T0: 1,5,9,... ) اینجا درج شود.]

### 1.16. چالش‌های Shared Address Space در solver
#### 1.16.1. وابستگی داده در Gauss-Seidel
به‌روزرسانی in-place وابستگی read-after-write ایجاد می‌کند (یک thread نوشته را بخواند قبل از تمام threads).

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 29: وابستگی‌های red-black در شبکه شطرنجی اینجا درج شود.]

**راه‌حل: Red-Black Gauss-Seidel:** شبکه را شطرنجی رنگ کنید؛ redها مستقل از blackها. دو فاز: red سپس black.

```cpp
// Phase 1: Red cells
for (int i = 1; i <= n; i += 2) {
    for (int j = 1 + (i%2); j <= n; j += 2) {  // red positions
        // update using black neighbors (unchanged)
    }
}
// Phase 2: Black cells
// similar, using updated red
```

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 30: کد red-black با دو فاز و color indexing اینجا درج شود.]

#### 1.16.2. همگام‌سازی
barrier بین فازها برای اطمینان از تکمیل فاز قبلی.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 31: کد با barrier(myBarrier, NUM_THREADS) اینجا درج شود.]

#### 1.16.3. کاهش diff (Reduction)
جمع diffهای محلی به جهانی با lock برای atomicity.

```cpp
lock(myLock);
diff += myDiff;  // critical section
unlock(myLock);
```

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 32: کد با lock برای global diff اینجا درج شود.]

#### 1.16.4. وابستگی‌های بین-فازی
سه barrier: یکی برای sync فاز، یکی برای reduction، یکی برای check done.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 33: کد کامل با سه barrier و lock/unlock اینجا درج شود.]

### 1.17. بهینه‌سازی solver: کاهش به یک barrier
برای کاهش overhead، وابستگی‌ها را با diff arrays چرخشی حذف کنید.

```cpp
int n; bool done = false;
LOCK myLock; BARRIER myBarrier;
float diff[3];  // سه کپی برای چرخش
float* A = allocate(n+2, n+2);  // flatten array

void solve(float* A) {
    float myDiff; int index = 0;
    diff[0] = 0.0f;
    barrier(myBarrier, NUM_PROCESSORS);  // init sync
    while (!done) {
        myDiff = 0.0f;
        // compute locally into myDiff (red-black phases)
        // no lock needed if atomic add used
        diff[index] += myDiff;
        diff[(index + 1) % 3] = 0.0f;  // reset next
        barrier(myBarrier, NUM_PROCESSORS);  // single barrier
        if (diff[index] / (n * n) < TOLERANCE) break;
        index = (index + 1) % 3;  // rotate
    }
}
```
ایده: هر تکرار از diff متفاوت استفاده می‌کند، وابستگی را حذف (trade-off: footprint بیشتر برای parallelism).

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 45: کد centralized barrier با diff[3] و index rotation اینجا درج شود.]

### 1.18. مشخص کردن وابستگی‌های دقیق‌تر
Barriers ساده اما coarse هستند (تمام کار تا این نقطه تمام شود). برای عملکرد بهتر، وابستگی‌های خاص مشخص کنید:

مثال: thread T0 نتیجه x تولید می‌کند، T1 مصرف.
```cpp
// T0: produce then signal
x = 1;
flag = 1;  // set flag
// more independent work...

// T1: independent stuff then wait
while (flag == 0);  // spin on flag
print(x);  // consume
```
این یک message queue طول 1 است؛ flag وابستگی point-to-point را نشان می‌دهد، که finer از barrier است و parallelism بیشتری آشکار می‌کند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 46: نمودار T0/T1 با flag dependency و queue visualization اینجا درج شود.]

### 1.19. مقایسه مدل‌ها در solver: Data-Parallel vs Shared Address Space
- **Data-Parallel:** sync با barrier ضمنی در انتهای forall؛ ارتباط ضمنی در loads/stores؛ primitives مانند reduce برای diff.
- **Shared Address Space:** locks برای shared vars؛ barriers برای phases؛ ارتباط ضمنی اما با race risks.

در data-parallel، reduce_add(&global_diff, &my_diff) ساده‌تر است.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 47: جدول مقایسه sync/comm در دو مدل اینجا درج شود.]

### 1.20. بیان solver در مدل Message Passing
در message passing، هر thread آدرس فضایی خصوصی دارد (no shared vars). پیکربندی نمونه: کلاستری دو workstation.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 48: کلی message passing model اینجا درج شود.]
[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 49: cluster دو computer با network اینجا درج شود.]

شبکه به allocations خصوصی تقسیم می‌شود (4 threads، 4 arrays).

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 50: 4 thread address spaces با private arrays اینجا درج شود.]

### 1.21. تکرار داده و Ghost Cells
برای وابستگی‌های edge، ghost cells (سلول‌های حاشیه‌ای تکراری) از remote spaces کپی می‌شوند.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 51: grid با ghost cells (white dots) و send row مثال (T1/T3 to T2) اینجا درج شود.]

### 1.22. پیاده‌سازی کامل Message Passing solver
```cpp
int N;
int tid = get_thread_id();
int rows_per_thread = N / get_num_threads();
float* localA = allocate(rows_per_thread + 2, N + 2);  // +2 برای ghosts
// initialize localA

void solve() {
    bool done = false;
    while (!done) {
        float my_diff = 0.0f;
        // exchange ghosts with neighbors
        if (tid != 0)
            send(&localA[1, 0], sizeof(float) * (N + 2), tid - 1, MSG_ID_ROW);  // send up
        if (tid != get_num_threads() - 1)
            send(&localA[rows_per_thread, 0], sizeof(float) * (N + 2), tid + 1, MSG_ID_ROW);  // send down
        if (tid != 0)
            recv(&localA[0, 0], sizeof(float) * (N + 2), tid - 1, MSG_ID_ROW);  // recv up
        if (tid != get_num_threads() - 1)
            recv(&localA[rows_per_thread + 1, 0], sizeof(float) * (N + 2), tid + 1, MSG_ID_ROW);  // recv down
        // compute on local (red-black if needed)
        for (int i = 1; i < rows_per_thread + 1; i++) {
            for (int j = 1; j < N + 1; j++) {
                float prev = localA[i * (N + 2) + j];  // flatten
                localA[i * (N + 2) + j] = 0.2f * (localA[(i - 1) * (N + 2) + j] + localA[i * (N + 2) + j] +
                                                  localA[(i + 1) * (N + 2) + j] + localA[i * (N + 2) + (j - 1)] +
                                                  localA[i * (N + 2) + (j + 1)]);
                my_diff += fabs(localA[i * (N + 2) + j] - prev);
            }
        }
        // reduce diff to tid 0
        if (tid != 0) {
            send(&my_diff, sizeof(float), 0, MSG_ID_DIFF);
            recv(&done, sizeof(bool), 0, MSG_ID_DONE);
        } else {
            float remote_diff;
            for (int i = 1; i < get_num_threads() - 1; i++) {
                recv(&remote_diff, sizeof(float), i, MSG_ID_DIFF);
                my_diff += remote_diff;
            }
            if (my_diff / (N * N) < TOLERANCE) done = true;
            for (int i = 1; i < get_num_threads() - 1; i++)
                send(&done, sizeof(bool), i, MSG_ID_DONE);
        }
    }
}
```
- send/recv سطرهای کامل برای ghosts؛ indexing محلی؛ reduction با send به root.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 52: کد message passing با send/recv ghosts و reduction اینجا درج شود.]

### 1.23. یادداشت‌های عملی بر Message Passing
- **محاسبه:** indexing نسبی به local space (نه global coordinates) برای سادگی.
- **ارتباط:** bulk transfer (rows نه cells) برای کارایی؛ MSG_IDها برای tagging.
- **همگام‌سازی:** send/recv ذاتاً sync است؛ primitives بالاتر مانند reduce_add(&global, &local) یا broadcast(root, &data) پیاده‌سازی‌شده با send/recv.
- مثال: reduce_add(0, &my_diff, sizeof(float)); broadcast(0, &done, sizeof(bool), MSG_DONE);

### 1.24. Send/Recv Synchronous (Blocking): جزئیات و چالش‌ها
در synchronous mode:
- **send():** برمی‌گردد پس از ack که داده در receiver space کپی شده.
- **recv():** برمی‌گردد پس از کپی داده به local buffer و ارسال ack.

این مدل ساده است اما deadlock-prone.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 54: timeline send/recv synchronous با copy و ack اینجا درج شود.]

**مشکل در solver:** اگر همه threads send کنند بدون recv، deadlock (circular wait).

### 1.25. حل Deadlock با Synchronous Send/Recv
odd/even threads: even send then recv، odd recv then send.

```cpp
if (tid % 2 == 0) {
    sendDown(); recvDown();  // even: send first
    sendUp(); recvUp();
} else {
    recvUp(); sendUp();  // odd: recv first
    recvDown(); sendDown();
}
// then compute
```

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 56: timeline T0-T5 با even/odd send/recv pattern اینجا درج شود.]

### 1.26. Send/Recv Non-Blocking (Asynchronous): برای overlap
- **send():** بلافاصله برمی‌گردد؛ buffer تا checksend modifiable نیست (concurrent copy).
- **recv():** handle برمی‌گرداند؛ checkrecv برای status.

این مدل overlap compute/comm را اجازه می‌دهد.

[شکل موجود در فایل s05_1_progbasics.pptx، اسلاید 57: timeline asynchronous با handles و check calls (red: concurrent) اینجا درج شود.]

### 1.27. خلاصه بخش 1
قانون Amdahl محدودیت‌های ذاتی موازی‌سازی را برجسته می‌کند. مراحل decomposition، assignment، orchestration و mapping interdependent هستند و باید با تمرکز روی وابستگی‌ها، locality و کاهش sync بهینه شوند. مثال solver نشان داد چگونه مدل‌ها (shared، message، data-parallel) چالش‌ها را حل می‌کنند. این دانش پایه برای بهینه‌سازی‌های پیشرفته است.

[شکل‌های پایانی: image1.png (اسلاید 58) تا image50.png (اسلاید 58) از s05_1_progbasics.pptx در انتهای بخش یا مکان‌های مرتبط درج شوند.]

---

## بخش 2: بهینه‌سازی عملکرد قسمت 1: توزیع کار و زمان‌بندی (Performance Optimization Part 1: Work Distribution and Scheduling)
**بر اساس فایل‌های: s05_2_progperf1.pptx (49 اسلاید) و s_s05_progperf1.pptx (62 اسلاید)**  
*ادغام: مطالب مشابه با جزئیات اضافی از s_s05_progperf1.pptx (مانند مثال‌های بیشتر و تصحیحات) گسترش یافته.*

این بخش بر چالش‌های عملی تمرکز دارد: چگونه کار را توزیع و زمان‌بندی کنیم تا منابع idle نشوند، اما overhead کم باشد. بازنویسی مبسوط شامل تحلیل trade-offها و نکات اندازه‌گیری است.

### 2.1. برنامه‌نویسی برای عملکرد بالا: فرآیند تکراری و اهداف
بهینه‌سازی برنامه‌های موازی یک فرآیند تکراری (iterative) است که شامل پالایش مداوم انتخاب‌ها در decomposition (تقسیم کار)، assignment (تخصیص)، و orchestration (هماهنگی) می‌شود. این فرآیند نه خطی، بلکه آزمایشی است: پیاده‌سازی، اندازه‌گیری، تحلیل bottleneck، و تنظیم.

اهداف کلیدی – که اغلب در تضاد هستند – عبارتند از:
- **تعادل بار روی منابع اجرایی:** همه cores/threads همزمان کار کنند تا idle time حداقل شود.
- **کاهش ارتباط:** جلوگیری از stallها ناشی از latency شبکه/حافظه (Amdahl-like برای comm).
- **کاهش overhead:** کار اضافی برای parallelism (مانند task creation) یا مدیریت assignment/کاهش ارتباط نباید speedup را خنثی کند.

فضای تکنیک‌ها غنی است: از static scheduling تا work stealing. نکته کلیدی: profiling (با ابزارهایی مانند VTune یا gprof) برای شناسایی hotspots ضروری است.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 2: نمودار iterative process و trade-off اهداف (balance, reduce comm, reduce overhead) اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 2: مشابه با "how to be l33t" title برای جذابیت اینجا درج شود.]

**TIP #1 (مبسوط):** همیشه ساده‌ترین راه‌حل (مانند static assignment) را اول پیاده کنید، عملکرد را روی ماشین هدف اندازه بگیرید (با wall-clock time و scaling plots)، و فقط اگر لازم (e.g., >20% idle) پیچیده‌تر کنید. "مقیاس‌پذیری" نسبی است – اگر فقط روی 4-core اجرا می‌کنید، hundreds tasks لازم نیست.

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 3: TIP #1 با quote "My solution scales" اینجا درج شود.]

### 2.2. تعادل بار کاری: اهمیت و تأثیر Amdahl
ایده‌آل: تمام processors در طول اجرا محاسبه کنند (simultaneous compute، finish همزمان). حتی imbalance کوچک (e.g., 20% کار بیشتر در یک thread) 20% زمان را سریال می‌کند، که طبق Amdahl speedup را محدود می‌کند (S=0.05 برای 20% imbalance).

مثال: P4 2x کار بیشتر → 50% runtime سریال (S=0.2، speedup max ~5x).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 4: جدول زمان P1-P4 با P4 20% longer (S=0.05) اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 4: P4 2x longer با 50% serial highlight اینجا درج شود.]

### 2.3. Static Assignment: مزایا و کاربردها
در static assignment، کار به threads از پیش (pre-determined) اختصاص می‌یابد – نه لزوماً compile-time، بلکه وقتی اندازه کار/threads شناخته‌شده (runtime params). مثال از PA1: grid cells برابر به threads (blocked: contiguous rows برای locality؛ interleaved: هر thread rows متناوب برای تعادل).

خواص: سادگی (no runtime decision)، overhead نزدیک صفر (فقط indexing math). اما برای workloads unpredictable نامناسب.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 5: blocked vs interleaved assignment در solver PA1 اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 5: T0-T3 blocked/interleaved visualization اینجا درج شود.]

**مثال PA1 Prog 1:** row-interleaved خوب بود زیرا locality row-wise را حفظ می‌کرد در حالی که load را تعادل می‌داد (no single thread hotspot).

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 8: مثال PA1 Prog 1 با row-interleaved grid اینجا درج شود.]

### 2.4. کاربردهای Static Assignment: سناریوهای عملی
- **کار یکنواخت:** 12 task هم‌اندازه، 4 processor: 3 task هر کدام (perfect balance).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 6: 12 tasks، 4 P، 3 each با زمان table اینجا درج شود.]

- **کار نابرابر اما شناخته‌شده:** assign tasks برای کار کلی برابر (e.g., long tasks به threads کمتربار).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 7: jobs unequal known، assign balanced time اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 7: assign equal tasks برای average balance اینجا درج شود.]

- **آمار شناخته‌شده:** average cost یکسان، static کافی.

### 2.5. Semi-Static Assignment: برای تغییرات آهسته
وقتی کار near-term predictable (گذشته پیش‌بینی آینده)، periodically profile و readjust. assignment "static" بین readjustments.

مثال‌ها:
- **Adaptive Mesh:** mesh تغییر با حرکت object/flow، اما آهسته؛ color assignment به processors.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 8: adaptive mesh image با color assignment اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 9: colored regions mesh cells به workers اینجا درج شود.]

- **Particle Simulation:** redistribute particles با حرکت (slow motion، infrequent redistribute).

این روش overhead کم با adaptability ترکیب می‌کند.

### 2.6. Dynamic Assignment: برای Unpredictable Workloads
وقتی زمان tasks یا تعداد ناشناخته، runtime assign برای توزیع خوب. مثال primality test (unknown time per i).

**Sequential:**
```cpp
int N = 1024;
int* x = new int[N];
bool* is_prime = new bool[N];
// init x
for (int i = 0; i < N; i++) {
    is_prime[i] = test_primality(x[i]);  // independent, unknown time
}
```

**Parallel (SPMD, shared space):**
```cpp
LOCK counter_lock;
int counter = 0;
while (1) {
    int i;
    lock(counter_lock);
    i = counter++;  // atomic fetch-and-inc
    unlock(counter_lock);
    if (i >= N) break;
    is_prime[i] = test_primality(x[i]);
}
```

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 9: کد dynamic با lock/unlock counter اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 10: مشابه با atomic_incr note اینجا درج شود.]

**مثال PA1 Prog 3:** شکستن به many ISPC tasks عملکرد را بهبود داد زیرا dynamic balance imbalance را جبران کرد.

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 12: PA1 Prog 3 ISPC tasks breakdown اینجا درج شود.]

### 2.7. Dynamic Assignment با Shared Work Queue
threads pull tasks از shared queue، push new work. فرض independence tasks.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 10: T1-T4 pulling from shared queue، sub-problems list اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 11: مشابه با "for now, assume independent" note اینجا درج شود.]

### 2.8. Granularity Task: Fine vs Coarse
**Fine (1 task = 1 element):**
```cpp
const int N = 1024;
LOCK counter_lock; int counter = 0;
while (1) {
    int i;
    lock(counter_lock); i = counter++; unlock(counter_lock);
    if (i >= N) break;
    is_prime[i] = test_primality(x[i]);  // small task
}
```
- مزایا: balance عالی (many tasks).
- معایب: high sync cost (frequent critical section، serialization – Amdahl serial overhead).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 11: زمان critical vs task، "IS IT a problem?" اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 13: مشابه با serialization highlight اینجا درج شود.]

**Coarse (1 task = GRANULARITY elements):**
```cpp
const int GRANULARITY = 10;
while (1) {
    int i;
    lock(counter_lock); i = counter; counter += GRANULARITY; unlock(counter_lock);
    if (i >= N) break;
    int end = min(i + GRANULARITY, N);
    for (int j = i; j < end; j++)
        is_prime[j] = test_primality(x[j]);  // batch process
}
```
- sync 10x کمتر؛ trade-off: ممکن imbalance اگر GRANULARITY نامناسب.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 13: زمان reduced critical section اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 14: مشابه با min(end) note اینجا درج شود.]

### 2.9. انتخاب Granularity: Trade-offها
- **Small tasks:** many > P processors برای dynamic balance؛ motivates fine.
- **Large tasks:** few tasks برای min overhead (queue mgmt)؛ motivates coarse.
- ایده‌آل: workload/machine-specific (profile برای پیدا کردن؛ ~8-16x P tasks رایج).

### 2.10. زمان‌بندی هوشمند: جلوگیری از Imbalance
در shared queue، left-to-right assign imbalance اگر tasks نابرابر (long task last → slop).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 15: 16 tasks left-to-right، imbalance time table اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 16: مشابه با "What happens if...?" query اینجا درج شود.]

**Long last:** idle threads در انتها.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 16: long task last، P4 idle اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 17: "Potential for load imbalance!" با Done! highlight اینجا درج شود.]

**حل 1: Smaller tasks** – long pole نسبت به total کوتاه‌تر، اما sync overhead ↑؛ اگر sequential، impossible.

**حل 2: Long first** – thread long fewer tasks اما کار برابر؛ نیاز predictability (sort tasks by estimated cost).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 17: long first scheduling، balanced done اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 18: "Schedule long tasks first" با reduced slop اینجا درج شود.]

### 2.11. Distributed Queues: کاهش Sync Overhead
به جای single queue (contention بالا)، per-thread queues: pull/push local؛ steal when empty.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 18: T1-T4 local queues، steal arrow اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 19: set of queues، "Steal!" visualization اینجا درج شود.]

- sync/comm فقط در steal (rare برای balance).
- locality: producer-consumer (thread tasks خود را پردازش).
- چالش‌ها: victim selection (random)، amount steal (half queue)، termination (all empty + quiescence)، local access fast (lock-free deque).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 19: locality benefits، challenges list اینجا درج شود.]

### 2.12. Task Dependencies: Non-Independent Work
queues می‌توانند وابستگی‌ها را مدیریت کنند: task تا satisfaction deps assign نمی‌شود.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 20: dependency graph با arrows، task handles اینجا درج شود.]

```cpp
foo_handle = enqueue_task(foo);  // independent
bar_handle = enqueue_task(bar, foo_handle);  // dep on foo
// scheduler waits for foo complete before bar
```

runtime deps را track می‌کند (مانند TBB یا OpenMP tasks with depend).

### 2.13. خلاصه بهینه‌سازی توزیع کار
چالش: balance بدون idle، اما low-cost (min compute/sync overhead). Static/dynamic continuum: از knowledge upfront (static) برای کاهش cost، dynamic برای unpredictability. مسائل decomposition/assignment/orchestration inter-related؛ profile برای تصمیم.

### 2.14. زمان‌بندی Fork-Join Parallelism: الگوهای رایج
Fork-join برای divide-and-conquer (مانند quicksort، merge sort) ایده‌آل است.

**Data Parallelism:** same ops on data elements.
```cpp
// CUDA bulk launch
foo<<<numBlocks, threadsPerBlock>>>(A, B);

// map higher-order
map(foo, A, B);

// OpenMP
#pragma omp parallel for
for (int i = 0; i < N; i++) B[i] = foo(A[i]);

// ISPC foreach
foreach (i = 0 ... N) B[i] = foo(A[i]);

// ISPC launch
launch[numTasks] myFooTask(A, B);
```

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 24: data parallelism diagram با foo() on elements اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 23: مشابه با code snippets اینجا درج شود.]

**Explicit Threads:**
```cpp
// C++ threads
std::thread thread[NUM_HW_EXEC_CONTEXTS];
for (int i = 0; i < NUM_HW_EXEC_CONTEXTS; i++) {
    thread[i] = std::thread(myFunction, A, B);
}
for (int i = 0; i < num_cores; i++) thread[i].join();
```

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 24: C++ thread creation/join code اینجا درج شود.]

### 2.15. Divide-and-Conquer: Quick Sort مثال
```cpp
void quick_sort(int* begin, int* end) {
    if (begin >= end - 1) return;  // base case
    int* middle = partition(begin, end);  // pivot and sort
    quick_sort(begin, middle);  // left sub
    quick_sort(middle + 1, end);  // right sub, independent!
}
```
subcalls independent – perfect fork.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 26: quick_sort tree با independent work highlight اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 26: مشابه با "independent work!" arrow اینجا درج شود.]

Fork: spawn subthreads؛ Join: wait all.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 27: fork-join diagram با qs subtrees اینجا درج شود.]

### 2.16. Cilk Plus: Abstraction Fork-Join
Cilk spawn/sync abstraction ساده parallelism expose می‌کند.

```cpp
cilk_spawn foo(); bar(); cilk_sync;  // foo || bar

cilk_spawn foo(); cilk_spawn bar(); cilk_sync;  // both spawned

cilk_spawn foo(); cilk_spawn bar(); cilk_spawn fizz(); buzz(); cilk_sync;  // multiple
```
spawn: may concurrent؛ sync: all complete before return. overhead کمتر با one spawn.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 28: spawn/sync trees (first: one spawn, second: two) اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 28: مشابه با parallel execution paths اینجا درج شود.]

Abstraction: spawn scheduling unspecified؛ sync constraint.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 29: abstraction vs impl note اینجا درج شود.]

### 2.17. Parallel Quicksort در Cilk
```cpp
void quick_sort(int* begin, int* end) {
    if (begin >= end - PARALLEL_CUTOFF)  // threshold for sequential
        std::sort(begin, end);  // avoid overhead small problems
    else {
        int* middle = partition(begin, end);
        cilk_spawn quick_sort(begin, middle);  // fork left
        quick_sort(middle + 1, end);  // run right
        // sync implicit on return
    }
}
```
cutoff: sequential اگر small (overhead spawn > benefit).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 30: quicksort tree با std::sort leaves و part() nodes اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 30: مشابه با parallel cutoff note اینجا درج شود.]

**نوشتن Fork-Join:** expose independent با spawn؛ parallel slack ~8 (work > cores برای balance)؛ not too much (fine granularity overhead).

### 2.18. Runtime Cilk: Worker Pool
به جای create/join per spawn (heavyweight)، pool fixed workers (تعداد cores، lazy init).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 33: 8-thread pool برای quad-core HT، while loop workers اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 33: thread 0-7 pool با work_exists loop اینجا درج شود.]

### 2.19. اجرای Spawn: Child-First Policy
در spawn foo(); bar(); sync: run child first (call foo)، queue continuation (bar).

serial equiv: run foo then bar (stack implicit).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 35: serial stack with foo then bar اینجا درج شود.]

idle steal continuation.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 36-39: مراحل queue bar، steal by T1، execute bar اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 35-39: مشابه با thread call stack و work queues اینجا درج شود.]

### 2.20. Child vs Continuation First: Trade-off
- **Continuation first (child stealing):** breadth-first؛ loop spawn O(N) space queued.
- **Child first (continuation stealing):** depth-first؛ loop one continuation (i++)؛ space ≤ T * single-stack؛ execution order like serial.

انتخاب: child first برای locality و space.

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 40-42: loop spawn comparison (breadth vs depth) اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 40-43: steal continuation i=1, i=2 با proof space اینجا درج شود.]

### 2.21. Quicksort Scheduling مثال
200 elements: queue cont subranges؛ steal top (larger chunks).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 44: T0 queue cont 101-200 etc، T1/T2 working اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 44: "What work to steal? top or bottom" query اینجا درج شود.]

### 2.22. Work Stealing Impl: Deque per Worker
deque: local tail push/pop (fast)؛ steal head (no contention).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 45-46: steal from head، post-steal queues اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 45-47: cont 51-100 steal، then distributed conts اینجا درج شود.]

random victim؛ top steal: amortize cost over large work، max locality (child-first + local tree).

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 48: random choice victim با top steal benefits اینجا درج شود.]

### 2.23. Child-First برای Divide-and-Conquer
recursive_for depth-first fill machine faster.

```cpp
void recursive_for(int start, int end) {
    while (start <= end - GRANULARITY) {
        int mid = (end - start) / 2;
        cilk_spawn recursive_for(start, mid);  // spawn left
        start = mid;  // continue right (depth)
    }
    for (int i = start; i < end; i++) foo(i);  // leaf
}
```
vs flat spawn loop (parallel generate، quick fill).

[شکل موجود در فایل s05_2_progperf1.pptx، اسلاید 48: recursive tree (N/2,3N/4 etc) vs flat foo(0..N-1) اینجا درج شود.]
[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 49: code snippets و tree comparison اینجا درج شود.]

### 2.24. Sync Impl: Descriptors برای Stealing
بدون steal: sync no-op (local complete).
با steal: block descriptor track spawned/done count.

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 50-60: مراحل sync stealing با id=A، spawn/done counts، steals، complete bar اینجا درج شود.] (جزئیات کامل: near-complete loop، steals، updates، resume continuation).

### 2.25. Greedy Join در Cilk
threads always steal if idle؛ no wait at join (look for work). initiator ممکن bar را اجرا نکند؛ overhead فقط on steal (infrequent با large chunks).

[شکل موجود در فایل s_s05_progperf1.pptx، اسلاید 61: greedy policy note اینجا درج شود.]

### 2.26. خلاصه Fork-Join
Fork-join divide-and-conquer را طبیعی express می‌کند (Cilk/OpenMP). runtime work stealing locality-aware: child-first، deque head steal، random victim، greedy join. برای PA: Tree-parallel Fibonacci.cpp run کنید.

[شکل‌های image1.png (اسلاید 49/62) تا image70.png از هر دو فایل در انتها یا مرتبط درج شوند.]

---

## بخش 3: پیاده‌سازی همگام‌سازی (Implementing Synchronization)
**بر اساس فایل: s05_3_synchronization.pptx (29 اسلاید)**

این بخش primitives sync را عمیق بررسی می‌کند، از locks تا barriers، با تمرکز روی efficiency در multi-core. بازنویسی شامل تحلیل traffic، scalability، و trade-offها است.

### 3.1. موضوع: Primitives Sync کارآمد
امروز: locks (test-and-set, MCS)، atomics، transactions برای mutual exclusion؛ barriers/flags برای signaling. فازها: acquire (gain access)، wait (spin/block)، release (enable others).

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 1: title slide با Prof. Amin Eskandari اینجا درج شود.]

### 3.2. سه فاز Sync Event
- **Acquire:** thread تلاش برای resource (e.g., lock check).
- **Waiting:** algorithm انتظار (busy or block).
- **Release:** enable others (e.g., unlock).

### 3.3. Busy Waiting: مزایا/معایب
spin: while(!condition) {} – CPU waste، اما low overhead اگر short wait.

در OS/15-213: بد به دلیل power waste؛ اما در parallel critical، OK اگر no oversubscribe.

### 3.4. Blocking Sync: Desirable برای Long Waits
اگر no progress، block و preempt (OS schedule other). مثال pthread_mutex_lock.

### 3.5. Busy vs Blocking: When to Choose
Busy preferable اگر overhead schedule > wait expected؛ tail latency؛ no other tasks need CPU. در parallel perf app: no oversubscribe، multi-thread latency hide OK. مثال: pthread_spin_lock، OSSpinLock.

### 3.6. Locks Impl
#### 3.6.1. Simple but Wrong Lock
```
lock: ld R0, mem[addr]
      cmp R0, #0
      bnz lock
      st mem[addr], #1
unlock: st mem[addr], #0
```
race: both load 0, both store 1.

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 8: race example P0/P1 loads اینجا درج شود.]

#### 3.6.2. Test-and-Set (T&S)
atomic ts: load and set if 0.
```
lock: ts R0, mem[addr]
      bnz R0, lock
unlock: st mem[addr], #0
```

#### 3.6.3. T&S Coherence Traffic
BusRdX/invalidate per attempt؛ high under contention.

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 10: P0-P2 T&S sequence با BusRdX/inval اینجا درج شود.]
[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 11: extended traffic with multiple fails اینجا درج شود.]

#### 3.6.4. T&S Performance
lock/unlock time ↑ با P به دلیل bus contention؛ critical section هم slow.

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 12: time (us) vs processors، credit Culler et al. اینجا درج شود.]

#### 3.6.5. Desirable Lock Properties
low latency (uncontended quick)، low traffic (O(P) not O(P²))، scalable، low storage، fair (FIFO, no starvation).

T&S: low latency low-contention، high traffic/poor scale، 1 int، no fair.

#### 3.6.6. Test-and-Test-and-Set (TTAS)
```cpp
void Lock(int* lock) {
    while (1) {
        while (*lock != 0);  // spin read local
        if (test_and_set(lock) == 0) return;  // then atomic
    }
}
void Unlock(volatile int* lock) { *lock = 0; }
```
spin local read (no traffic) تا release، then T&S.

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 14: TTAS code با comments اینجا درج شود.]

#### 3.6.7. TTAS Traffic
one inval per waiter per release (O(P))؛ less than T&S (per test inval).

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 15: P1-P3 traffic با many local reads، BusRd on release اینجا درج شود.]
[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 16: extended with T&S fails minimized اینجا درج شود.]

#### 3.6.8. TTAS Characteristics
higher uncontended latency (extra test loop)؛ much less traffic، better scale؛ 1 int، no fair.

#### 3.6.9. T&S with Backoff
delay exponential on fail.
```cpp
void Lock(volatile int* l) {
    int amount = 1;
    while (1) {
        if (test_and_set(l) == 0) return;
        delay(amount);
        amount *= 2;  // expo backoff
    }
}
```
same uncontended؛ less traffic (no constant poll)؛ scale better؛ unfair (late arrivers less delay).

#### 3.6.10. Ticket Lock
queue-like: ticket inc، spin on serving.
```cpp
struct lock { volatile int next_ticket, now_serving; };

void Lock(lock* l) {
    int my_ticket = atomic_increment(&l->next_ticket);  // get ticket
    while (my_ticket != l->now_serving);  // spin
}
void Unlock(lock* l) { l->now_serving++; }  // next
```
read-only spin؛ O(P) traffic per release.

#### 3.6.11. Array-Based Lock
per-processor status array (padded lines).
```cpp
struct lock { volatile padded_int status[P]; volatile int head; };
int my_element;

void Lock(lock* l) {
    my_element = atomic_circ_increment(&l->head);  // assign slot
    while (l->status[my_element] == 1);  // spin local
}
void Unlock(lock* l) {
    l->status[my_element] = 0;  // release self
    l->status[circ_next(my_element)] = 0;  // ? wait, code has =1 then next=0 – clarify: set current unlocked, next ready?
}
```
O(1) traffic/release؛ space O(P)؛ circ inc overhead.

#### 3.6.12. x86 CMPXCHG
atomic compare-exchange برای building locks.
```
lock cmpxchg dst, src  // if dst == acc, dst=src, ZF=1; else ZF=0, acc=dst
```

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 21: cmpxchg flowchart با accumulator/dst/src اینجا درج شود.]

#### 3.6.13. MCS Lock (Queue-Based)
local nodes queue؛ glock tail.
```
AcquireQLock(*glock, *mlock) {
    mlock->next = NULL; mlock->state = UNLOCKED;
    ATOMIC { prev = *glock; *glock = mlock; }
    if (prev == NULL) return;
    mlock->state = LOCKED; prev->next = mlock;
    while (mlock->state == LOCKED);  // spin local
}

ReleaseQLock(*glock, *mlock) {
    do {
        if (mlock->next == NULL) {
            x = CMPXCHG(glock, mlock, NULL);
            if (x == mlock) return;
        } else mlock->next->state = UNLOCKED; return;
    } while (1);
}
```
fair، local spin، O(1) traffic.

### 3.7. Barriers Impl
#### 3.7.1. Centralized (Counter)
```cpp
struct Barrier_t { LOCK lock; int counter=0; int flag; };  // flag padded

void Barrier(Barrier_t* b, int p) {
    lock(b->lock);
    if (b->counter == 0) b->flag = 0;
    int num = ++b->counter;
    unlock(b->lock);
    if (num == p) { b->counter=0; b->flag=1; }
    else while (b->flag == 0);
}
```
works but leave issues (threads stuck if not reset).

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 24: code with if first thread clears flag اینجا درج شود.]

#### 3.7.2. Correct Centralized
add leave_counter=P init؛ first arriver wait leave==P before clear.

```cpp
struct { LOCK lock; int arrive=0, leave=P; int flag; };

void Barrier(Barrier_t* b, int p) {
    lock(b->lock);
    if (b->arrive == 0) {
        if (b->leave == p) b->flag = 0;
        else { unlock; while (b->leave != p); lock; b->flag=0; }
    }
    int num = ++b->arrive;
    unlock;
    if (num == p) { b->arrive=0; b->leave=1; b->flag=1; }
    else { while (!b->flag); lock; b->leave++; unlock; }
}
```
wait all leave before next entry.

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 25: correct code with leave_counter logic اینجا درج شود.]

#### 3.7.3. Sense Reversal
local_sense flip per call؛ wait flag==local_sense (last set to it).

```cpp
int local_sense = 0;
void Barrier(Barrier_t* b, int p) {
    local_sense = 1 - local_sense;
    lock(b->lock);
    int num = ++b->counter;
    unlock;
    if (num == p) { b->counter=0; b->flag = local_sense; }
    else while (b->flag != local_sense);
}
```
one spin (predict flip).

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 26: sense reversal code با local_sense flip اینجا درج شود.]

#### 3.7.4. Traffic Centralized
O(P) writes (2P lock/counter, 2 last, P-1 reads)؛ span O(P) serialization.

#### 3.7.5. Combining Tree
parallel inc parents to root (lg P span)؛ release notify children from root. better topology (not bus).

[شکل موجود در فایل s05_3_synchronization.pptx، اسلاید 29: centralized vs combining tree diagrams (high contention vs low) اینجا درج شود.]
[شکل‌های image1.png (اسلاید 29) تا image9.png (اسلاید 29) در انتها درج شوند.]

**پایان جزوه.** این نسخه جامع، خودکفا و آماده استفاده است. برای سؤالات بیشتر، خوشحال می‌شم کمک کنم!
